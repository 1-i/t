\section{Forme matriciali e operatori}

\subsection{Rappresentazione vettoriale degli stati}

Come abbiamo visto precedentemente, uno stato arbitrario $\ket{\psi}$ può essere scritto nella forma:

	\[
		\ket{\psi} = \ket{g_1} \braket{g_1}{\psi} + \ket{g_2} \braket{g_2}{\psi} + ... = \sum_i \ket{g_i} \braket{g_i}{\psi}
	\]

Analogamente a come i vettori di un qualsiasi spazio vettoriale possono essere scritti, una volta scelta una base, esclusivamente in funzione dei loro coefficienti, possiamo fissare la base $B = (\ket{g_1}, \ket{g_2},... \ket{g_n})$ e rappresentare un ket $\ket{\psi}$ con una matrice colonna:

	\begin{equation} \label{eq:ketColumnMatrix}
		\ket{\psi} = \left ( \begin{array}{c}
				\braket{g_1}{\psi} \\
				\braket{g_2}{\psi} \\
				\braket{g_3}{\psi} \\
				... \\
				\braket{g_n}{\psi}
			\end{array}
		\right )
	\end{equation}

Essendo $B$ una base ortonormale, evidentemente si avrà che per ciascuno dei componenti della base $\ket{g_k}$:

	\begin{equation}
		\ket{g_k} = \left ( \begin{array}{c}
				\braket{g_1}{g_k} \\
				\braket{g_2}{g_k} \\
				\braket{g_3}{g_k} \\
				... \\
				\braket{g_n}{g_k}
			\end{array}
		\right ) = \left ( \begin{array}{c}
				\delta_{1k} \\
				\delta_{2k} \\
				\delta_{3k} \\
				... \\
				\delta_{nk}
			\end{array}
		\right ) = e_k
	\end{equation}

È possibile rappresentare in maniera analoga un \textit{bra}? Una scelta naturale della base è la base duale di $B$, $B^*$. Infatti, come abbiamo determinato dal raffronto delle \eqref{eq:ketsSuperposition}, \eqref{eq:brasSuperposition} i coefficienti di $\bra{\psi}$ in $B^*$ saranno i complessi coniugati dei coefficienti di $\ket{\psi}$ in $B$. È però più conveniente rappresentare i \textit{bra} come matrici riga. Scrivendo:

	\begin{equation} \label{eq:braRowMatrix}
		\bra{\varphi} = \left ( \begin{array}{c c c c c}
			\braket{\varphi}{g_1} & \braket{\varphi}{g_2} & \braket{\varphi}{g_3} & ... & \braket{\varphi}{g_n}
		\end{array} \right )
	\end{equation}

Possiamo rappresentare il prodotto interno $\braket{\varphi}{\psi}$ come prodotto della matrice riga che rappresenta $\bra{\varphi}$ e della matrice colonna che rappresenta $\ket{\psi}$, richiedendo ovviamente che la prima sia espressa in base $B^*$ e la seconda in base $B$:

	\begin{equation} \label{eq:braketMatrixProduct}
		\braket{\varphi}{\psi} =
		\left ( \begin{array}{c c c c c}
			\braket{\varphi}{g_1} & \braket{\varphi}{g_2} & \braket{\varphi}{g_3} & ... & \braket{\varphi}{g_n}
		\end{array} \right )_{B^*}
		\left ( \begin{array}{c}
				\braket{g_1}{\psi} \\
				\braket{g_2}{\psi} \\
				\braket{g_3}{\psi} \\
				... \\
				\braket{g_n}{\psi}
			\end{array}
		\right )_B
	\end{equation}

È facile verificare che il prodotto tra le due matrici nella \eqref{eq:braketMatrixProduct} è uguale per definizione al prodotto interno $\left \langle \varphi, \psi \right \rangle$ definito nella \eqref{eq:innerProduct}.

La \eqref{eq:ketColumnMatrix} e la \eqref{eq:braRowMatrix} ci consentono di scrivere anche il prodotto esterno definito nella \eqref{eq:outerProduct} in maniera analoga come:

	\begin{equation} \label{eq:ketbraMatrixProduct}
		\ket{\psi}\bra{\varphi} =
		\left ( \begin{array}{c}
				\braket{g_1}{\psi} \\
				\braket{g_2}{\psi} \\
				\braket{g_3}{\psi} \\
				... \\
				\braket{g_n}{\psi}
			\end{array}
		\right )_B
		\left ( \begin{array}{c c c c c}
			\braket{\varphi}{g_1} & \braket{\varphi}{g_2} & \braket{\varphi}{g_3} & ... & \braket{\varphi}{g_n}
		\end{array} \right )_{B^*}
	\end{equation}

Svolgendo il prodotto ad RHS nella \eqref{eq:ketbraMatrixProduct} otteniamo, come lecito aspettarsi da un operatore lineare, una matrice $M \in \mathbb{C}^{n \times n}$:

	\begin{equation}
		M_{ij} = \braket{g_i}{\psi} \braket{\varphi}{g_j}
	\end{equation}

Per verificare la consistenza di questa notazione, consideriamo un vettore $\ket{\zeta}$ e calcoliamo $M \ket{\zeta}$:

	\begin{equation} \label{eq:matrixDer1}
		M \ket{\zeta} = \left ( \begin{array}{c}
				\braket{g_1}{\psi} \sum_i \braket{\varphi}{g_i} \braket{g_i}{\alpha} \\
				... \\
				\braket{g_k}{\psi} \sum_i \braket{\varphi}{g_i} \braket{g_i}{\alpha} \\
				... \\
				\braket{g_n}{\psi} \sum_i \braket{\varphi}{g_i} \braket{g_i}{\alpha} \\
			\end{array}
		\right )
	\end{equation}

La somma

	\begin{equation}
		\sum_i \braket{\varphi}{g_i} \braket{g_i}{\alpha}
	\end{equation}

si riscrive per linearità come

	\begin{equation} \label{eq:matrixDer2}
		\bra{\varphi} \sum_i \ket{g_i} \braket{g_i}{\alpha}
	\end{equation}

Ma per la \eqref{eq:ketsSuperposition} si deve avere

	\begin{equation}
		\sum_i \ket{g_i} \braket{g_i}{\alpha} = \ket{\alpha}
	\end{equation}

Quindi la \eqref{eq:matrixDer2} diventa

	\begin{equation} \label{eq:matrixDer3}
		\sum_i \braket{\varphi}{g_i} \braket{g_i}{\alpha} = \braket{\varphi}{\alpha}
	\end{equation}

Sostituendo la \eqref{eq:matrixDer3} nella \eqref{eq:matrixDer1}:

	\begin{equation}
			M \ket{\zeta} = \left ( \begin{array}{c}
				\braket{g_1}{\psi} \braket{\varphi}{\alpha} \\
				... \\
				\braket{g_k}{\psi} \braket{\varphi}{\alpha} \\
				... \\
				\braket{g_n}{\psi} \braket{\varphi}{\alpha} \\
			\end{array}
		\right ) = \braket{\varphi}{\alpha} \left ( \begin{array}{c}
				\braket{g_1}{\psi} \\
				... \\
				\braket{g_k}{\psi} \\
				... \\
				\braket{g_n}{\psi} \\
			\end{array}
		\right )
	\end{equation}

Quest'ultima si riscrive come:

	\begin{equation}
		M \ket{\zeta} = \braket{\varphi}{\alpha} \ket{\psi}
	\end{equation}

la quale conferma che $M$ si comporta effettivamente come l'operatore prodotto esterno definito nella \eqref{eq:outerProduct}.

Più in generale, consideriamo il problema della rappresentazione in forma matriciale di un operatore lineare qualsiasi $\hat{A}$. Scriviamo l'equazione generica:

	\begin{equation} \label{eq:operatorGeneric}
		\hat{A} \ket{\psi} = \ket{\phi}
	\end{equation}

Come già ampiamente giustificato, data una base $B = (g_1, g_2, ..., g_n)$ possiamo scrivere:

	\begin{equation}
		\ket{\psi} = \sum_i \ket{g_i} \braket{g_i}{\psi}
	\end{equation}

Sostituendo la precedente nella \eqref{eq:operatorGeneric}:

	\begin{equation}
		\hat{A} \left ( \sum_i \ket{g_i} \braket{g_i}{\psi} \right ) = \ket{\varphi}
	\end{equation}

Applicando un $\bra{g_k}$:

	\begin{equation} \label{eq:operatorAsMatrix}
		\left ( \sum_i \mel{g_k}{\hat{A}}{g_i} \braket{g_i}{\psi} \right ) = \braket{g_k}{\varphi}
	\end{equation}

La \eqref{eq:operatorAsMatrix} si può riscrivere come:

	\begin{equation}
		M \left ( \begin{array}{c}
				\braket{g_1}{\psi} \\
				\braket{g_2}{\psi} \\
				... \\
				\braket{g_n}{\psi} \\
			\end{array}
		\right ) = \left ( \begin{array}{c}
				\braket{g_1}{\varphi} \\
				\braket{g_2}{\varphi} \\
				... \\
				\braket{g_n}{\varphi} \\
			\end{array}
		\right )
	\end{equation}

Dove

	\begin{equation}
		M_{ij} = \mel{g_i}{\hat{A}}{g_j} 
	\end{equation}

Ma

	\begin{equation}
		\left ( \begin{array}{c}
				\braket{g_1}{\psi} \\
				\braket{g_2}{\psi} \\
				... \\
				\braket{g_n}{\psi} \\
			\end{array}
		\right ) = \ket{\psi}
	\end{equation}

e

	\begin{equation}
		\left ( \begin{array}{c}
				\braket{g_1}{\varphi} \\
				\braket{g_2}{\varphi} \\
				... \\
				\braket{g_n}{\varphi} \\
			\end{array}
		\right ) = \ket{\varphi}
	\end{equation}

Abbiamo dunque dedotto che un generico operatore $\hat{A}$ può essere scritto come una matrice

	\begin{equation} \label{eq:matricialFormOperator}
		M_{ij} = \mel{g_i}{\hat{A}}{g_j} 
	\end{equation}

	Possiamo trovare anche un altro interessante risultato. Infatti, per quanto stabilito nella \eqref{eq:proofHard0}, se $\hat{A}$ è autoaggiunto e soddisfa:
	
	\[
		\hat{A} \ket{\psi} = \ket{\varphi}
	\]
	
deve necessariamente valere anche:

	\[
		\bra{\psi} \hat{A} = \bra{\varphi}
	\]

Prendiamo ora il prodotto della prima con $\bra{\chi}$:

	\begin{equation}
		\mel{\chi}{\hat{A}}{\psi} = \braket{\chi}{\varphi}
	\end{equation}

E il prodotto della seconda con $\ket{\chi}$:

	\begin{equation}
		\mel{\psi}{\hat{A}}{\chi}
	\end{equation}

Ora siccome $\braket{\chi}{\varphi} = \braket{\chi}{\varphi}^*$ abbiamo

	\begin{equation} \label{eq:hermitianConj}
		\mel{\psi}{\hat{A}}{\chi} = \mel{\chi}{\hat{A}}{\psi}^*
	\end{equation}

	Dal momento che la \eqref{eq:hermitianConj} vale per ogni scelta di $(\bra{\chi}, \bra{\psi})$, vale in particolare per due generici stati $(g_i, g_j)$, abbiamo dunque:

	\begin{equation}
		\mel{g_i}{\hat{A}}{g_j} = \mel{g_j}{\hat{A}}{g_i}^*
	\end{equation}

Essendo i due termini esattamente gli elementi delle matrici associate, deduciamo che la rappresentazione matriciale di un operatore autoaggiunto è uguale alla sua coniugata trasposta, consistentemente con la nomenclatura impiegata dall'algebra lineare.
